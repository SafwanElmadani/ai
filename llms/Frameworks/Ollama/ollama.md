https://github.com/ollama/ollama

ollama allows you to run llm locally. It streamlines the process of installing LLMs.


## Notes:
- Im running ollama server manually so I can control where things go.
- using this `OLLAMA_HOST=0.0.0.0:11434 ollama serve` to start it, so it can connect to the open webui docker container.
