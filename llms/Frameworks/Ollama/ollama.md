https://github.com/ollama/ollama

ollama allows you to run llm locally. It streamlines the process of installing LLMs.
- to save model in different dir: `OLLAMA_MODELS=/home/safwan/wd_ssd/ollama/models`


## Notes:
- Im running ollama server manually so I can control where things go.
- using this `OLLAMA_HOST=0.0.0.0:11434 ollama serve` to start it, so it can connect to the open webui docker container.
